time="2023-09-25T00:10:37+02:00" level=info msg="Acquiring lock file /var/lib/rancher/k3s/data/.lock"
time="2023-09-25T00:10:37+02:00" level=info msg="Preparing data dir /var/lib/rancher/k3s/data/58cbe03d7ea7167b3921ba5332233a89c43d76fc99dbc8e0d6fdc5dcec19ee9d"
time="2023-09-25T00:10:38+02:00" level=info msg="Starting k3s v1.26.5+k3s1 (7cefebea)"
time="2023-09-25T00:10:38+02:00" level=info msg="Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s"
time="2023-09-25T00:10:38+02:00" level=info msg="Configuring database table schema and indexes, this may take a moment..."
time="2023-09-25T00:10:38+02:00" level=info msg="Database tables and indexes are up to date"
time="2023-09-25T00:10:38+02:00" level=info msg="Kine available at unix://kine.sock"
time="2023-09-25T00:10:38+02:00" level=info msg="generated self-signed CA certificate CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38.036992416 +0000 UTC notAfter=2033-09-21 22:10:38.036992416 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:kube-proxy signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:k3s-controller signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="generated self-signed CA certificate CN=k3s-server-ca@1695593438: notBefore=2023-09-24 22:10:38.04262995 +0000 UTC notAfter=2033-09-21 22:10:38.04262995 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=kube-apiserver signed by CN=k3s-server-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="generated self-signed CA certificate CN=k3s-request-header-ca@1695593438: notBefore=2023-09-24 22:10:38.043260825 +0000 UTC notAfter=2033-09-21 22:10:38.043260825 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1695593438: notBefore=2023-09-24 22:10:38.043837824 +0000 UTC notAfter=2033-09-21 22:10:38.043837824 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1695593438: notBefore=2023-09-24 22:10:38.047244903 +0000 UTC notAfter=2033-09-21 22:10:38.047244903 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=info msg="Saving cluster bootstrap data to datastore"
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
time="2023-09-25T00:10:38+02:00" level=warning msg="dynamiclistener [::]:6443: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
time="2023-09-25T00:10:38+02:00" level=info msg="Active TLS secret / (ver=) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-mamauraiserver:mamauraiserver listener.cattle.io/fingerprint:SHA1=D63363D8C3CBB5BDAD84AE1018BF492DF2147B6A]"
time="2023-09-25T00:10:38+02:00" level=info msg="Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --feature-gates=JobTrackingWithFinalizers=true --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
time="2023-09-25T00:10:38+02:00" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259"
time="2023-09-25T00:10:38+02:00" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --feature-gates=JobTrackingWithFinalizers=true --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true"
time="2023-09-25T00:10:38+02:00" level=info msg="Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false"
time="2023-09-25T00:10:38+02:00" level=info msg="Server node token is available at /var/lib/rancher/k3s/server/token"
time="2023-09-25T00:10:38+02:00" level=info msg="To join server node to cluster: k3s server -s https://192.168.141.250:6443 -t ${SERVER_NODE_TOKEN}"
time="2023-09-25T00:10:38+02:00" level=info msg="Agent node token is available at /var/lib/rancher/k3s/server/agent-token"
time="2023-09-25T00:10:38+02:00" level=info msg="To join agent node to cluster: k3s agent -s https://192.168.141.250:6443 -t ${AGENT_NODE_TOKEN}"
time="2023-09-25T00:10:38+02:00" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2023-09-25T00:10:38+02:00" level=info msg="Run: k3s kubectl"
W0925 00:10:38.212838    9334 feature_gate.go:241] Setting GA feature gate JobTrackingWithFinalizers=true. It will be removed in a future release.
I0925 00:10:38.213108    9334 server.go:569] external host was not specified, using 192.168.56.110
I0925 00:10:38.213471    9334 server.go:171] Version: v1.26.5+k3s1
I0925 00:10:38.213554    9334 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
time="2023-09-25T00:10:38+02:00" level=info msg="Waiting for API server to become available"
I0925 00:10:38.393761    9334 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0925 00:10:38.393988    9334 plugins.go:161] Loaded 12 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0925 00:10:38.397410    9334 shared_informer.go:270] Waiting for caches to sync for node_authorizer
W0925 00:10:38.405202    9334 genericapiserver.go:660] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0925 00:10:38.405795    9334 instance.go:277] Using reconciler: lease
time="2023-09-25T00:10:38+02:00" level=info msg="certificate CN=mamauraiserver signed by CN=k3s-server-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:38 +0000 UTC"
I0925 00:10:38.820796    9334 instance.go:621] API group "internal.apiserver.k8s.io" is not enabled, skipping.
time="2023-09-25T00:10:39+02:00" level=info msg="certificate CN=system:node:mamauraiserver,O=system:nodes signed by CN=k3s-client-ca@1695593438: notBefore=2023-09-24 22:10:38 +0000 UTC notAfter=2024-09-23 22:10:39 +0000 UTC"
I0925 00:10:39.201533    9334 instance.go:621] API group "resource.k8s.io" is not enabled, skipping.
time="2023-09-25T00:10:39+02:00" level=info msg="Module overlay was already loaded"
time="2023-09-25T00:10:39+02:00" level=info msg="Module nf_conntrack was already loaded"
W0925 00:10:39.265483    9334 genericapiserver.go:660] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.265635    9334 genericapiserver.go:660] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.266432    9334 genericapiserver.go:660] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.267957    9334 genericapiserver.go:660] Skipping API autoscaling/v2beta1 because it has no resources.
W0925 00:10:39.268055    9334 genericapiserver.go:660] Skipping API autoscaling/v2beta2 because it has no resources.
W0925 00:10:39.268996    9334 genericapiserver.go:660] Skipping API batch/v1beta1 because it has no resources.
W0925 00:10:39.269896    9334 genericapiserver.go:660] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.270843    9334 genericapiserver.go:660] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.270950    9334 genericapiserver.go:660] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.273181    9334 genericapiserver.go:660] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.273278    9334 genericapiserver.go:660] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.274030    9334 genericapiserver.go:660] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.274122    9334 genericapiserver.go:660] Skipping API node.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.274220    9334 genericapiserver.go:660] Skipping API policy/v1beta1 because it has no resources.
time="2023-09-25T00:10:39+02:00" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
time="2023-09-25T00:10:39+02:00" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
time="2023-09-25T00:10:39+02:00" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
time="2023-09-25T00:10:39+02:00" level=warning msg="SELinux is enabled on this host, but k3s has not been started with --selinux - containerd SELinux support is disabled"
time="2023-09-25T00:10:39+02:00" level=info msg="Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log"
W0925 00:10:39.287374    9334 genericapiserver.go:660] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.287477    9334 genericapiserver.go:660] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.288136    9334 genericapiserver.go:660] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.288229    9334 genericapiserver.go:660] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.289953    9334 genericapiserver.go:660] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.295619    9334 genericapiserver.go:660] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.295763    9334 genericapiserver.go:660] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
time="2023-09-25T00:10:39+02:00" level=info msg="Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd"
W0925 00:10:39.328684    9334 genericapiserver.go:660] Skipping API apps/v1beta2 because it has no resources.
W0925 00:10:39.332806    9334 genericapiserver.go:660] Skipping API apps/v1beta1 because it has no resources.
W0925 00:10:39.334067    9334 genericapiserver.go:660] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.334178    9334 genericapiserver.go:660] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
W0925 00:10:39.335124    9334 genericapiserver.go:660] Skipping API events.k8s.io/v1beta1 because it has no resources.
W0925 00:10:39.349757    9334 genericapiserver.go:660] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0925 00:10:39.771878    9334 secure_serving.go:210] Serving securely on 127.0.0.1:6444
I0925 00:10:39.772085    9334 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0925 00:10:39.772226    9334 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt::/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
I0925 00:10:39.772366    9334 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0925 00:10:39.772752    9334 available_controller.go:423] Starting AvailableConditionController
I0925 00:10:39.772845    9334 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0925 00:10:39.772943    9334 controller.go:83] Starting OpenAPI AggregationController
I0925 00:10:39.773029    9334 controller.go:80] Starting OpenAPI V3 AggregationController
I0925 00:10:39.773317    9334 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0925 00:10:39.773418    9334 shared_informer.go:270] Waiting for caches to sync for cluster_authentication_trust_controller
I0925 00:10:39.773659    9334 gc_controller.go:78] Starting apiserver lease garbage collector
I0925 00:10:39.773753    9334 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt::/var/lib/rancher/k3s/server/tls/client-auth-proxy.key"
I0925 00:10:39.774235    9334 autoregister_controller.go:141] Starting autoregister controller
I0925 00:10:39.774318    9334 cache.go:32] Waiting for caches to sync for autoregister controller
I0925 00:10:39.774533    9334 controller.go:121] Starting legacy_token_tracking_controller
I0925 00:10:39.774616    9334 shared_informer.go:270] Waiting for caches to sync for configmaps
I0925 00:10:39.774905    9334 apf_controller.go:361] Starting API Priority and Fairness config controller
I0925 00:10:39.775154    9334 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0925 00:10:39.775239    9334 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0925 00:10:39.775369    9334 customresource_discovery_controller.go:288] Starting DiscoveryController
I0925 00:10:39.775463    9334 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0925 00:10:39.775674    9334 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/server/tls/client-ca.crt"
I0925 00:10:39.775787    9334 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/rancher/k3s/server/tls/request-header-ca.crt"
I0925 00:10:39.777291    9334 crdregistration_controller.go:111] Starting crd-autoregister controller
I0925 00:10:39.777445    9334 shared_informer.go:270] Waiting for caches to sync for crd-autoregister
I0925 00:10:39.777621    9334 controller.go:85] Starting OpenAPI controller
I0925 00:10:39.777753    9334 controller.go:85] Starting OpenAPI V3 controller
I0925 00:10:39.777841    9334 naming_controller.go:291] Starting NamingConditionController
I0925 00:10:39.777969    9334 establishing_controller.go:76] Starting EstablishingController
I0925 00:10:39.778103    9334 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0925 00:10:39.778204    9334 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0925 00:10:39.778294    9334 crd_finalizer.go:266] Starting CRDFinalizer
I0925 00:10:39.828982    9334 controller.go:615] quota admission added evaluator for: namespaces
E0925 00:10:39.831785    9334 controller.go:156] Unable to perform initial Kubernetes service initialization: Service "kubernetes" is invalid: spec.clusterIPs: Invalid value: []string{"10.43.0.1"}: failed to allocate IP 10.43.0.1: cannot allocate resources of type serviceipallocations at this time
I0925 00:10:39.873936    9334 cache.go:39] Caches are synced for AvailableConditionController controller
I0925 00:10:39.874098    9334 shared_informer.go:277] Caches are synced for cluster_authentication_trust_controller
I0925 00:10:39.874404    9334 cache.go:39] Caches are synced for autoregister controller
I0925 00:10:39.874702    9334 shared_informer.go:277] Caches are synced for configmaps
I0925 00:10:39.875154    9334 apf_controller.go:366] Running API Priority and Fairness config worker
I0925 00:10:39.875259    9334 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0925 00:10:39.875363    9334 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0925 00:10:39.877541    9334 shared_informer.go:277] Caches are synced for crd-autoregister
I0925 00:10:39.879490    9334 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0925 00:10:39.897697    9334 shared_informer.go:277] Caches are synced for node_authorizer
time="2023-09-25T00:10:40+02:00" level=info msg="containerd is now running"
time="2023-09-25T00:10:40+02:00" level=info msg="Connecting to proxy" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2023-09-25T00:10:40+02:00" level=info msg="Handling backend connection request [mamauraiserver]"
time="2023-09-25T00:10:40+02:00" level=info msg="Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=cgroupfs --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --healthz-bind-address=127.0.0.1 --hostname-override=mamauraiserver --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --kubelet-cgroups=/k3s --node-ip=192.168.56.110 --node-labels= --pod-infra-container-image=rancher/mirrored-pause:3.6 --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/etc/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key"
time="2023-09-25T00:10:40+02:00" level=info msg="Waiting to retrieve kube-proxy configuration; server is not ready: https://127.0.0.1:6443/v1-k3s/readyz: 500 Internal Server Error"
I0925 00:10:40.620054    9334 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0925 00:10:40.790044    9334 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0925 00:10:40.794651    9334 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0925 00:10:40.794886    9334 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0925 00:10:40.917104    9334 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0925 00:10:40.925508    9334 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0925 00:10:40.936428    9334 alloc.go:327] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.43.0.1]
W0925 00:10:40.937793    9334 lease.go:251] Resetting endpoints for master service "kubernetes" to [192.168.56.110]
I0925 00:10:40.938155    9334 controller.go:615] quota admission added evaluator for: endpoints
I0925 00:10:40.939310    9334 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
Flag --cloud-provider has been deprecated, will be removed in 1.25 or later, in favor of removing cloud provider code from Kubelet.
Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.27. Image garbage collector will get sandbox image information from CRI.
I0925 00:10:41.352828    9334 server.go:197] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0925 00:10:41.355480    9334 server.go:407] "Kubelet version" kubeletVersion="v1.26.5+k3s1"
I0925 00:10:41.355680    9334 server.go:409] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 00:10:41.357149    9334 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/rancher/k3s/agent/client-ca.crt"
W0925 00:10:41.359706    9334 machine.go:65] Cannot read vendor id correctly, set empty.
I0925 00:10:41.360385    9334 server.go:654] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0925 00:10:41.361096    9334 container_manager_linux.go:267] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0925 00:10:41.361356    9334 container_manager_linux.go:272] "Creating Container Manager object based on Node Config" nodeConfig={RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName:/k3s KubeletOOMScoreAdj:-999 ContainerRuntime: CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:cgroupfs KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] CPUManagerPolicy:none CPUManagerPolicyOptions:map[] ExperimentalTopologyManagerScope:container CPUManagerReconcilePeriod:10s ExperimentalMemoryManagerPolicy:None ExperimentalMemoryManagerReservedMemory:[] ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none ExperimentalTopologyManagerPolicyOptions:map[]}
I0925 00:10:41.362262    9334 topology_manager.go:134] "Creating topology manager with policy per scope" topologyPolicyName="none" topologyScopeName="container"
I0925 00:10:41.362456    9334 container_manager_linux.go:308] "Creating device plugin manager"
I0925 00:10:41.362760    9334 state_mem.go:36] "Initialized new in-memory state store"
I0925 00:10:41.368028    9334 kubelet.go:398] "Attempting to sync node with API server"
I0925 00:10:41.368211    9334 kubelet.go:286] "Adding static pod path" path="/var/lib/rancher/k3s/agent/pod-manifests"
I0925 00:10:41.368679    9334 kubelet.go:297] "Adding apiserver pod source"
I0925 00:10:41.368959    9334 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0925 00:10:41.372225    9334 kuberuntime_manager.go:244] "Container runtime initialized" containerRuntime="containerd" version="v1.7.1-k3s1" apiVersion="v1"
W0925 00:10:41.372846    9334 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0925 00:10:41.374316    9334 server.go:1181] "Started kubelet"
I0925 00:10:41.379164    9334 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0925 00:10:41.381785    9334 server.go:161] "Starting to listen" address="0.0.0.0" port=10250
I0925 00:10:41.382655    9334 server.go:451] "Adding debug handlers to kubelet server"
I0925 00:10:41.384986    9334 volume_manager.go:293] "Starting Kubelet Volume Manager"
I0925 00:10:41.388770    9334 desired_state_of_world_populator.go:151] "Desired state populator starts to run"
E0925 00:10:41.412351    9334 cri_stats_provider.go:455] "Failed to get the info of the filesystem with mountpoint" err="unable to find data in memory cache" mountpoint="/var/lib/rancher/k3s/agent/containerd/io.containerd.snapshotter.v1.overlayfs"
E0925 00:10:41.412636    9334 kubelet.go:1386] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
E0925 00:10:41.427246    9334 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"mamauraiserver\" not found" node="mamauraiserver"
I0925 00:10:41.427662    9334 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0925 00:10:41.427763    9334 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0925 00:10:41.427861    9334 state_mem.go:36] "Initialized new in-memory state store"
I0925 00:10:41.428783    9334 policy_none.go:49] "None policy: Start"
I0925 00:10:41.429333    9334 memory_manager.go:169] "Starting memorymanager" policy="None"
I0925 00:10:41.429435    9334 state_mem.go:35] "Initializing new in-memory state store"
I0925 00:10:41.442016    9334 manager.go:455] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0925 00:10:41.444234    9334 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
E0925 00:10:41.448740    9334 eviction_manager.go:261] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"mamauraiserver\" not found"
I0925 00:10:41.485943    9334 kubelet_node_status.go:70] "Attempting to register node" node="mamauraiserver"
I0925 00:10:41.495063    9334 kubelet_node_status.go:73] "Successfully registered node" node="mamauraiserver"
time="2023-09-25T00:10:41+02:00" level=info msg="Annotations and labels have been set successfully on node: mamauraiserver"
time="2023-09-25T00:10:41+02:00" level=info msg="Starting flannel with backend vxlan"
I0925 00:10:41.503852    9334 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv4
I0925 00:10:41.528123    9334 kubelet_network_linux.go:63] "Initialized iptables rules." protocol=IPv6
I0925 00:10:41.528273    9334 status_manager.go:176] "Starting to sync pod status with apiserver"
I0925 00:10:41.528375    9334 kubelet.go:2113] "Starting kubelet main sync loop"
E0925 00:10:41.528499    9334 kubelet.go:2137] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
time="2023-09-25T00:10:41+02:00" level=info msg="Kube API server is now running"
time="2023-09-25T00:10:41+02:00" level=info msg="ETCD server is now running"
time="2023-09-25T00:10:41+02:00" level=info msg="k3s is up and running"
time="2023-09-25T00:10:41+02:00" level=info msg="Waiting for cloud-controller-manager privileges to become available"
W0925 00:10:41.843174    9334 feature_gate.go:241] Setting GA feature gate JobTrackingWithFinalizers=true. It will be removed in a future release.
I0925 00:10:42.094243    9334 serving.go:355] Generated self-signed cert in-memory
time="2023-09-25T00:10:42+02:00" level=info msg="Applying CRD helmcharts.helm.cattle.io"
I0925 00:10:42.222752    9334 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
I0925 00:10:42.253186    9334 controllermanager.go:182] Version: v1.26.5+k3s1
I0925 00:10:42.253321    9334 controllermanager.go:184] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 00:10:42.256424    9334 secure_serving.go:210] Serving securely on 127.0.0.1:10257
I0925 00:10:42.257223    9334 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0925 00:10:42.257320    9334 shared_informer.go:270] Waiting for caches to sync for RequestHeaderAuthRequestController
I0925 00:10:42.257414    9334 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0925 00:10:42.257524    9334 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0925 00:10:42.257622    9334 shared_informer.go:270] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 00:10:42.257720    9334 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0925 00:10:42.257821    9334 shared_informer.go:270] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2023-09-25T00:10:42+02:00" level=info msg="Applying CRD helmchartconfigs.helm.cattle.io"
I0925 00:10:42.279541    9334 shared_informer.go:270] Waiting for caches to sync for tokens
time="2023-09-25T00:10:42+02:00" level=info msg="Applying CRD addons.k3s.cattle.io"
I0925 00:10:42.288402    9334 controller.go:615] quota admission added evaluator for: serviceaccounts
I0925 00:10:42.290186    9334 controllermanager.go:622] Started "deployment"
W0925 00:10:42.290302    9334 controllermanager.go:587] "service" is disabled
I0925 00:10:42.290574    9334 deployment_controller.go:154] "Starting controller" controller="deployment"
I0925 00:10:42.290665    9334 shared_informer.go:270] Waiting for caches to sync for deployment
time="2023-09-25T00:10:42+02:00" level=info msg="Waiting for CRD addons.k3s.cattle.io to become available"
I0925 00:10:42.321992    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for replicasets.apps
I0925 00:10:42.322419    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
I0925 00:10:42.322781    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for limitranges
I0925 00:10:42.323197    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
I0925 00:10:42.323421    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for podtemplates
I0925 00:10:42.323812    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for daemonsets.apps
I0925 00:10:42.324044    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for statefulsets.apps
I0925 00:10:42.324287    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for cronjobs.batch
I0925 00:10:42.324497    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for ingresses.networking.k8s.io
I0925 00:10:42.327268    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for serviceaccounts
I0925 00:10:42.327412    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
I0925 00:10:42.327512    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for endpointslices.discovery.k8s.io
I0925 00:10:42.327619    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for helmcharts.helm.cattle.io
I0925 00:10:42.327715    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for addons.k3s.cattle.io
I0925 00:10:42.327886    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for deployments.apps
I0925 00:10:42.327988    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
I0925 00:10:42.328089    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for csistoragecapacities.storage.k8s.io
I0925 00:10:42.328188    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for jobs.batch
I0925 00:10:42.328279    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
I0925 00:10:42.328377    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for helmchartconfigs.helm.cattle.io
I0925 00:10:42.328474    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for endpoints
I0925 00:10:42.328574    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for controllerrevisions.apps
I0925 00:10:42.328676    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
I0925 00:10:42.328793    9334 controllermanager.go:622] Started "resourcequota"
I0925 00:10:42.328952    9334 resource_quota_controller.go:277] Starting resource quota controller
I0925 00:10:42.329036    9334 shared_informer.go:270] Waiting for caches to sync for resource quota
I0925 00:10:42.329283    9334 resource_quota_monitor.go:295] QuotaMonitor running
I0925 00:10:42.357419    9334 shared_informer.go:277] Caches are synced for RequestHeaderAuthRequestController
I0925 00:10:42.357734    9334 shared_informer.go:277] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 00:10:42.357949    9334 shared_informer.go:277] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0925 00:10:42.368865    9334 controllermanager.go:622] Started "namespace"
I0925 00:10:42.369093    9334 namespace_controller.go:195] Starting namespace controller
I0925 00:10:42.369179    9334 shared_informer.go:270] Waiting for caches to sync for namespace
I0925 00:10:42.370499    9334 apiserver.go:52] "Watching apiserver"
I0925 00:10:42.372690    9334 controllermanager.go:622] Started "attachdetach"
I0925 00:10:42.372869    9334 attach_detach_controller.go:328] Starting attach detach controller
I0925 00:10:42.372951    9334 shared_informer.go:270] Waiting for caches to sync for attach detach
I0925 00:10:42.374969    9334 controllermanager.go:622] Started "replicaset"
W0925 00:10:42.375055    9334 controllermanager.go:587] "route" is disabled
I0925 00:10:42.375213    9334 replica_set.go:201] Starting replicaset controller
I0925 00:10:42.375295    9334 shared_informer.go:270] Waiting for caches to sync for ReplicaSet
I0925 00:10:42.377226    9334 controllermanager.go:622] Started "endpoint"
I0925 00:10:42.377343    9334 endpoints_controller.go:178] Starting endpoint controller
I0925 00:10:42.377422    9334 shared_informer.go:270] Waiting for caches to sync for endpoint
I0925 00:10:42.379650    9334 controllermanager.go:622] Started "csrsigning"
I0925 00:10:42.379757    9334 shared_informer.go:277] Caches are synced for tokens
I0925 00:10:42.379981    9334 certificate_controller.go:112] Starting certificate controller "csrsigning-kubelet-serving"
I0925 00:10:42.380073    9334 shared_informer.go:270] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I0925 00:10:42.380165    9334 certificate_controller.go:112] Starting certificate controller "csrsigning-kubelet-client"
I0925 00:10:42.380291    9334 shared_informer.go:270] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I0925 00:10:42.380381    9334 certificate_controller.go:112] Starting certificate controller "csrsigning-kube-apiserver-client"
I0925 00:10:42.380467    9334 shared_informer.go:270] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I0925 00:10:42.380627    9334 certificate_controller.go:112] Starting certificate controller "csrsigning-legacy-unknown"
I0925 00:10:42.380711    9334 shared_informer.go:270] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I0925 00:10:42.380834    9334 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0925 00:10:42.380964    9334 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0925 00:10:42.381090    9334 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/client-ca.key"
I0925 00:10:42.381214    9334 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt::/var/lib/rancher/k3s/server/tls/server-ca.key"
I0925 00:10:42.383422    9334 controllermanager.go:622] Started "statefulset"
I0925 00:10:42.383548    9334 stateful_set.go:152] Starting stateful set controller
I0925 00:10:42.383626    9334 shared_informer.go:270] Waiting for caches to sync for stateful set
I0925 00:10:42.385588    9334 controllermanager.go:622] Started "tokencleaner"
W0925 00:10:42.385671    9334 controllermanager.go:587] "cloud-node-lifecycle" is disabled
I0925 00:10:42.385758    9334 tokencleaner.go:111] Starting token cleaner controller
I0925 00:10:42.385837    9334 shared_informer.go:270] Waiting for caches to sync for token_cleaner
I0925 00:10:42.385920    9334 shared_informer.go:277] Caches are synced for token_cleaner
I0925 00:10:42.389653    9334 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
I0925 00:10:42.403966    9334 reconciler.go:41] "Reconciler: start to sync state"
I0925 00:10:42.533267    9334 controllermanager.go:622] Started "clusterrole-aggregation"
I0925 00:10:42.533578    9334 clusterroleaggregation_controller.go:188] Starting ClusterRoleAggregator
I0925 00:10:42.533741    9334 shared_informer.go:270] Waiting for caches to sync for ClusterRoleAggregator
I0925 00:10:42.682454    9334 controllermanager.go:622] Started "endpointslice"
I0925 00:10:42.682660    9334 endpointslice_controller.go:257] Starting endpoint slice controller
I0925 00:10:42.682785    9334 shared_informer.go:270] Waiting for caches to sync for endpoint_slice
time="2023-09-25T00:10:42+02:00" level=info msg="Done waiting for CRD addons.k3s.cattle.io to become available"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-21.2.1+up21.2.0.tgz"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-crd-21.2.1+up21.2.0.tgz"
time="2023-09-25T00:10:42+02:00" level=info msg="Failed to get existing traefik HelmChart" error="helmcharts.helm.cattle.io \"traefik\" not found"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/ccm.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/coredns.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/local-storage.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/traefik.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml"
time="2023-09-25T00:10:42+02:00" level=info msg="Tunnel server egress proxy mode: agent"
I0925 00:10:42.833741    9334 controllermanager.go:622] Started "replicationcontroller"
I0925 00:10:42.833977    9334 replica_set.go:201] Starting replicationcontroller controller
I0925 00:10:42.834086    9334 shared_informer.go:270] Waiting for caches to sync for ReplicationController
time="2023-09-25T00:10:42+02:00" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2023-09-25T00:10:42+02:00" level=info msg="Creating deploy event broadcaster"
I0925 00:10:42.981144    9334 controllermanager.go:622] Started "ttl"
I0925 00:10:42.981311    9334 ttl_controller.go:120] Starting TTL controller
I0925 00:10:42.981435    9334 shared_informer.go:270] Waiting for caches to sync for TTL
time="2023-09-25T00:10:43+02:00" level=info msg="Creating helm-controller event broadcaster"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting /v1, Kind=Node controller"
time="2023-09-25T00:10:43+02:00" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2023-09-25T00:10:43+02:00" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2023-09-25T00:10:43+02:00" level=info msg="Labels and annotations have been set successfully on node: mamauraiserver"
time="2023-09-25T00:10:43+02:00" level=warning msg="Unable to fetch coredns config map: configmaps \"coredns\" not found"
time="2023-09-25T00:10:43+02:00" level=info msg="Cluster dns configmap has been set successfully"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting /v1, Kind=ConfigMap controller"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting /v1, Kind=ServiceAccount controller"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting /v1, Kind=Secret controller"
I0925 00:10:43.131367    9334 node_lifecycle_controller.go:492] Controller will reconcile labels.
I0925 00:10:43.131499    9334 controllermanager.go:622] Started "nodelifecycle"
I0925 00:10:43.131633    9334 node_lifecycle_controller.go:527] Sending events to api server.
I0925 00:10:43.131749    9334 node_lifecycle_controller.go:538] Starting node controller
I0925 00:10:43.131851    9334 shared_informer.go:270] Waiting for caches to sync for taint
time="2023-09-25T00:10:43+02:00" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
time="2023-09-25T00:10:43+02:00" level=info msg="Starting batch/v1, Kind=Job controller"
time="2023-09-25T00:10:43+02:00" level=info msg="Updating TLS secret for kube-system/k3s-serving (count: 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-mamauraiserver:mamauraiserver listener.cattle.io/fingerprint:SHA1=D63363D8C3CBB5BDAD84AE1018BF492DF2147B6A]"
I0925 00:10:43.282170    9334 controllermanager.go:622] Started "ephemeral-volume"
I0925 00:10:43.282351    9334 controller.go:169] Starting ephemeral volume controller
I0925 00:10:43.282481    9334 shared_informer.go:270] Waiting for caches to sync for ephemeral
time="2023-09-25T00:10:43+02:00" level=info msg="Active TLS secret kube-system/k3s-serving (ver=238) (count 10): map[listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-192.168.56.110:192.168.56.110 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/cn-mamauraiserver:mamauraiserver listener.cattle.io/fingerprint:SHA1=D63363D8C3CBB5BDAD84AE1018BF492DF2147B6A]"
I0925 00:10:43.433448    9334 controllermanager.go:622] Started "endpointslicemirroring"
I0925 00:10:43.433807    9334 endpointslicemirroring_controller.go:211] Starting EndpointSliceMirroring controller
I0925 00:10:43.434079    9334 shared_informer.go:270] Waiting for caches to sync for endpoint_slice_mirroring
I0925 00:10:43.735325    9334 controllermanager.go:622] Started "horizontalpodautoscaling"
I0925 00:10:43.736527    9334 horizontal.go:181] Starting HPA controller
I0925 00:10:43.736967    9334 shared_informer.go:270] Waiting for caches to sync for HPA
I0925 00:10:43.882765    9334 controllermanager.go:622] Started "pvc-protection"
I0925 00:10:43.882930    9334 pvc_protection_controller.go:99] "Starting PVC protection controller"
I0925 00:10:43.883019    9334 shared_informer.go:270] Waiting for caches to sync for PVC protection
I0925 00:10:44.031787    9334 controllermanager.go:622] Started "pv-protection"
I0925 00:10:44.032044    9334 pv_protection_controller.go:75] Starting PV protection controller
I0925 00:10:44.032187    9334 shared_informer.go:270] Waiting for caches to sync for PV protection
I0925 00:10:44.187833    9334 controllermanager.go:622] Started "podgc"
I0925 00:10:44.188242    9334 gc_controller.go:102] Starting GC controller
I0925 00:10:44.188489    9334 shared_informer.go:270] Waiting for caches to sync for GC
I0925 00:10:44.385019    9334 controllermanager.go:622] Started "disruption"
I0925 00:10:44.385346    9334 disruption.go:424] Sending events to api server.
I0925 00:10:44.385642    9334 disruption.go:435] Starting disruption controller
I0925 00:10:44.385876    9334 shared_informer.go:270] Waiting for caches to sync for disruption
I0925 00:10:44.536787    9334 controllermanager.go:622] Started "job"
I0925 00:10:44.537061    9334 job_controller.go:191] Starting job controller
I0925 00:10:44.537250    9334 shared_informer.go:270] Waiting for caches to sync for job
I0925 00:10:44.687731    9334 controllermanager.go:622] Started "cronjob"
W0925 00:10:44.687991    9334 controllermanager.go:587] "bootstrapsigner" is disabled
I0925 00:10:44.688278    9334 cronjob_controllerv2.go:137] "Starting cronjob controller v2"
I0925 00:10:44.688435    9334 shared_informer.go:270] Waiting for caches to sync for cronjob
I0925 00:10:44.838334    9334 controllermanager.go:622] Started "persistentvolume-binder"
I0925 00:10:44.838718    9334 pv_controller_base.go:318] Starting persistent volume controller
I0925 00:10:44.838966    9334 shared_informer.go:270] Waiting for caches to sync for persistent volume
I0925 00:10:44.938560    9334 controller.go:615] quota admission added evaluator for: addons.k3s.cattle.io
I0925 00:10:44.945375    9334 event.go:294] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0925 00:10:45.163192    9334 event.go:294] "Event occurred" object="kube-system/ccm" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/ccm.yaml\""
I0925 00:10:45.183303    9334 controllermanager.go:622] Started "persistentvolume-expander"
I0925 00:10:45.183945    9334 expand_controller.go:340] Starting expand controller
I0925 00:10:45.184035    9334 shared_informer.go:270] Waiting for caches to sync for expand
I0925 00:10:45.216917    9334 serving.go:355] Generated self-signed cert in-memory
I0925 00:10:45.415213    9334 controllermanager.go:152] Version: v1.26.5+k3s1
I0925 00:10:45.416505    9334 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0925 00:10:45.418110    9334 controllermanager.go:622] Started "ttl-after-finished"
I0925 00:10:45.418278    9334 ttlafterfinished_controller.go:104] Starting TTL after finished controller
I0925 00:10:45.418364    9334 shared_informer.go:270] Waiting for caches to sync for TTL after finished
I0925 00:10:45.475383    9334 secure_serving.go:210] Serving securely on 127.0.0.1:10258
I0925 00:10:45.475659    9334 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0925 00:10:45.475750    9334 shared_informer.go:270] Waiting for caches to sync for RequestHeaderAuthRequestController
I0925 00:10:45.475852    9334 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0925 00:10:45.476035    9334 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0925 00:10:45.476136    9334 shared_informer.go:270] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 00:10:45.476235    9334 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0925 00:10:45.476335    9334 shared_informer.go:270] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
time="2023-09-25T00:10:45+02:00" level=info msg="Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=mamauraiserver --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables"
I0925 00:10:45.553852    9334 server.go:224] "Warning, all flags other than --config, --write-config-to, and --cleanup are deprecated, please begin using a config file ASAP"
I0925 00:10:45.566179    9334 controllermanager.go:622] Started "serviceaccount"
I0925 00:10:45.566574    9334 serviceaccounts_controller.go:111] Starting service account controller
I0925 00:10:45.566667    9334 shared_informer.go:270] Waiting for caches to sync for service account
I0925 00:10:45.567352    9334 node.go:163] Successfully retrieved node IP: 192.168.56.110
I0925 00:10:45.567442    9334 server_others.go:109] "Detected node IP" address="192.168.56.110"
time="2023-09-25T00:10:45+02:00" level=info msg="Creating service-controller event broadcaster"
I0925 00:10:45.572942    9334 server_others.go:176] "Using iptables Proxier"
I0925 00:10:45.573040    9334 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0925 00:10:45.573130    9334 server_others.go:184] "Creating dualStackProxier for iptables"
I0925 00:10:45.573216    9334 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0925 00:10:45.573326    9334 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0925 00:10:45.573576    9334 server.go:655] "Version info" version="v1.26.5+k3s1"
I0925 00:10:45.573731    9334 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 00:10:45.576098    9334 shared_informer.go:277] Caches are synced for RequestHeaderAuthRequestController
I0925 00:10:45.576266    9334 shared_informer.go:277] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 00:10:45.576384    9334 config.go:317] "Starting service config controller"
I0925 00:10:45.576461    9334 shared_informer.go:270] Waiting for caches to sync for service config
I0925 00:10:45.576546    9334 shared_informer.go:277] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0925 00:10:45.576644    9334 config.go:226] "Starting endpoint slice config controller"
I0925 00:10:45.576720    9334 shared_informer.go:270] Waiting for caches to sync for endpoint slice config
I0925 00:10:45.576907    9334 config.go:444] "Starting node config controller"
I0925 00:10:45.576983    9334 shared_informer.go:270] Waiting for caches to sync for node config
I0925 00:10:45.591298    9334 controllermanager.go:622] Started "daemonset"
I0925 00:10:45.591910    9334 daemon_controller.go:267] Starting daemon sets controller
I0925 00:10:45.591998    9334 shared_informer.go:270] Waiting for caches to sync for daemon sets
I0925 00:10:45.602755    9334 controllermanager.go:622] Started "root-ca-cert-publisher"
I0925 00:10:45.602880    9334 publisher.go:101] Starting root CA certificate configmap publisher
I0925 00:10:45.602963    9334 shared_informer.go:270] Waiting for caches to sync for crt configmap
I0925 00:10:45.605360    9334 controller.go:615] quota admission added evaluator for: deployments.apps
I0925 00:10:45.609340    9334 alloc.go:327] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.43.0.10]
I0925 00:10:45.609795    9334 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/coredns.yaml\""
I0925 00:10:45.613791    9334 event.go:294] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0925 00:10:45.627914    9334 event.go:294] "Event occurred" object="kube-system/local-storage" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/local-storage.yaml\""
I0925 00:10:45.631434    9334 controllermanager.go:622] Started "csrcleaner"
I0925 00:10:45.631545    9334 cleaner.go:82] Starting CSR cleaner controller
I0925 00:10:45.632157    9334 event.go:294] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0925 00:10:45.635365    9334 event.go:294] "Event occurred" object="kube-system/aggregated-metrics-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/aggregated-metrics-reader.yaml\""
I0925 00:10:45.638664    9334 event.go:294] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0925 00:10:45.642454    9334 event.go:294] "Event occurred" object="kube-system/auth-delegator" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-delegator.yaml\""
I0925 00:10:45.645686    9334 event.go:294] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0925 00:10:45.648296    9334 event.go:294] "Event occurred" object="kube-system/auth-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/auth-reader.yaml\""
I0925 00:10:45.651754    9334 event.go:294] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0925 00:10:45.663153    9334 event.go:294] "Event occurred" object="kube-system/metrics-apiservice" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-apiservice.yaml\""
I0925 00:10:45.677450    9334 shared_informer.go:277] Caches are synced for endpoint slice config
I0925 00:10:45.677574    9334 shared_informer.go:277] Caches are synced for node config
I0925 00:10:45.679882    9334 shared_informer.go:277] Caches are synced for service config
E0925 00:10:45.749938    9334 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0925 00:10:45.754832    9334 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
time="2023-09-25T00:10:45+02:00" level=info msg="Starting /v1, Kind=Node controller"
E0925 00:10:45.766177    9334 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0925 00:10:45.767120    9334 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
time="2023-09-25T00:10:45+02:00" level=info msg="Starting /v1, Kind=Pod controller"
E0925 00:10:45.776672    9334 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0925 00:10:45.841049    9334 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
time="2023-09-25T00:10:45+02:00" level=info msg="Starting apps/v1, Kind=DaemonSet controller"
E0925 00:10:45.853911    9334 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0925 00:10:45.854690    9334 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0925 00:10:45.856236    9334 controllermanager.go:311] Started "cloud-node-lifecycle"
I0925 00:10:45.856412    9334 controllermanager.go:311] Started "service"
W0925 00:10:45.856492    9334 controllermanager.go:288] "route" is disabled
I0925 00:10:45.856623    9334 controllermanager.go:311] Started "cloud-node"
time="2023-09-25T00:10:45+02:00" level=info msg="Starting discovery.k8s.io/v1, Kind=EndpointSlice controller"
I0925 00:10:45.857037    9334 node_lifecycle_controller.go:113] Sending events to api server
I0925 00:10:45.857141    9334 controller.go:227] Starting service controller
I0925 00:10:45.857219    9334 shared_informer.go:270] Waiting for caches to sync for service
I0925 00:10:45.857303    9334 node_controller.go:157] Sending events to api server.
I0925 00:10:45.857386    9334 node_controller.go:166] Waiting for informer caches to sync
I0925 00:10:45.958152    9334 node_controller.go:415] Initializing node mamauraiserver with cloud provider
I0925 00:10:45.958463    9334 shared_informer.go:277] Caches are synced for service
I0925 00:10:45.976167    9334 node_controller.go:484] Successfully initialized node mamauraiserver with cloud provider
I0925 00:10:45.979036    9334 event.go:294] "Event occurred" object="mamauraiserver" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="Synced" message="Node synced successfully"
I0925 00:10:46.137503    9334 serving.go:355] Generated self-signed cert in-memory
time="2023-09-25T00:10:46+02:00" level=info msg="Updated coredns node hosts entry [192.168.56.110 mamauraiserver]"
I0925 00:10:46.204578    9334 event.go:294] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0925 00:10:46.334314    9334 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.5+k3s1"
I0925 00:10:46.334473    9334 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 00:10:46.349715    9334 event.go:294] "Event occurred" object="kube-system/metrics-server-deployment" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-deployment.yaml\""
I0925 00:10:46.353000    9334 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0925 00:10:46.353414    9334 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0925 00:10:46.353509    9334 shared_informer.go:270] Waiting for caches to sync for RequestHeaderAuthRequestController
I0925 00:10:46.353682    9334 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0925 00:10:46.359040    9334 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0925 00:10:46.359188    9334 shared_informer.go:270] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 00:10:46.359326    9334 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0925 00:10:46.359438    9334 shared_informer.go:270] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0925 00:10:46.412523    9334 event.go:294] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0925 00:10:46.415262    9334 alloc.go:327] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs=map[IPv4:10.43.180.187]
I0925 00:10:46.415816    9334 event.go:294] "Event occurred" object="kube-system/metrics-server-service" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/metrics-server-service.yaml\""
I0925 00:10:46.454630    9334 shared_informer.go:277] Caches are synced for RequestHeaderAuthRequestController
I0925 00:10:46.460356    9334 shared_informer.go:277] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0925 00:10:46.460519    9334 shared_informer.go:277] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0925 00:10:46.663502    9334 handler_proxy.go:100] no RequestInfo found in the context
E0925 00:10:46.663896    9334 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0925 00:10:46.664309    9334 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0925 00:10:46.664797    9334 handler_proxy.go:100] no RequestInfo found in the context
E0925 00:10:46.665045    9334 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0925 00:10:46.665347    9334 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0925 00:10:46.818130    9334 event.go:294] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0925 00:10:46.833541    9334 event.go:294] "Event occurred" object="kube-system/resource-reader" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/metrics-server/resource-reader.yaml\""
I0925 00:10:47.212660    9334 event.go:294] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
I0925 00:10:47.224898    9334 event.go:294] "Event occurred" object="kube-system/rolebindings" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/rolebindings.yaml\""
time="2023-09-25T00:10:47+02:00" level=info msg="Tunnel authorizer set Kubelet Port 10250"
I0925 00:10:47.620128    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="ApplyingManifest" message="Applying manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0925 00:10:47.628601    9334 controller.go:615] quota admission added evaluator for: helmcharts.helm.cattle.io
I0925 00:10:47.638468    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Addon" apiVersion="k3s.cattle.io/v1" type="Normal" reason="AppliedManifest" message="Applied manifest at \"/var/lib/rancher/k3s/server/manifests/traefik.yaml\""
I0925 00:10:47.650288    9334 event.go:294] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0925 00:10:47.650615    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0925 00:10:47.667131    9334 controller.go:615] quota admission added evaluator for: jobs.batch
time="2023-09-25T00:10:47+02:00" level=error msg="error syncing 'kube-system/traefik': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik\" not found, requeuing"
time="2023-09-25T00:10:47+02:00" level=error msg="error syncing 'kube-system/traefik-crd': handler helm-controller-chart-registration: helmcharts.helm.cattle.io \"traefik-crd\" not found, requeuing"
I0925 00:10:47.681539    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0925 00:10:47.681693    9334 event.go:294] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0925 00:10:47.687269    9334 event.go:294] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0925 00:10:47.691529    9334 event.go:294] "Event occurred" object="kube-system/traefik-crd" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik-crd"
I0925 00:10:48.038064    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
I0925 00:10:48.057768    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="HelmChart" apiVersion="helm.cattle.io/v1" type="Normal" reason="ApplyJob" message="Applying HelmChart using Job kube-system/helm-install-traefik"
time="2023-09-25T00:10:48+02:00" level=info msg="Stopped tunnel to 127.0.0.1:6443"
time="2023-09-25T00:10:48+02:00" level=info msg="Connecting to proxy" url="wss://192.168.56.110:6443/v1-k3s/connect"
time="2023-09-25T00:10:48+02:00" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:6443/v1-k3s/connect"
time="2023-09-25T00:10:48+02:00" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
time="2023-09-25T00:10:48+02:00" level=info msg="Handling backend connection request [mamauraiserver]"
I0925 00:10:55.756107    9334 range_allocator.go:109] No Secondary Service CIDR provided. Skipping filtering out secondary service addresses.
I0925 00:10:55.756611    9334 controllermanager.go:622] Started "nodeipam"
I0925 00:10:55.756932    9334 node_ipam_controller.go:155] Starting ipam controller
I0925 00:10:55.757122    9334 shared_informer.go:270] Waiting for caches to sync for node
I0925 00:10:55.765351    9334 controllermanager.go:622] Started "garbagecollector"
I0925 00:10:55.765659    9334 garbagecollector.go:154] Starting garbage collector controller
I0925 00:10:55.765815    9334 shared_informer.go:270] Waiting for caches to sync for garbage collector
I0925 00:10:55.766096    9334 graph_builder.go:291] GraphBuilder running
I0925 00:10:55.768356    9334 controllermanager.go:622] Started "csrapproving"
I0925 00:10:55.771129    9334 certificate_controller.go:112] Starting certificate controller "csrapproving"
I0925 00:10:55.771283    9334 shared_informer.go:270] Waiting for caches to sync for certificate-csrapproving
I0925 00:10:55.774662    9334 shared_informer.go:270] Waiting for caches to sync for resource quota
W0925 00:10:55.793282    9334 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="mamauraiserver" does not exist
I0925 00:10:55.793551    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:10:55.793667    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:10:55.803068    9334 shared_informer.go:277] Caches are synced for crt configmap
W0925 00:10:55.813740    9334 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I0925 00:10:55.818542    9334 shared_informer.go:277] Caches are synced for TTL after finished
E0925 00:10:55.822277    9334 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0925 00:10:55.823580    9334 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0925 00:10:55.824433    9334 shared_informer.go:270] Waiting for caches to sync for garbage collector
I0925 00:10:55.832559    9334 shared_informer.go:277] Caches are synced for PV protection
I0925 00:10:55.832688    9334 shared_informer.go:277] Caches are synced for taint
I0925 00:10:55.832842    9334 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
W0925 00:10:55.832961    9334 node_lifecycle_controller.go:1053] Missing timestamp for Node mamauraiserver. Assuming now as a timestamp.
I0925 00:10:55.833133    9334 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I0925 00:10:55.833238    9334 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0925 00:10:55.833328    9334 taint_manager.go:211] "Sending events to api server"
I0925 00:10:55.833560    9334 event.go:294] "Event occurred" object="mamauraiserver" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node mamauraiserver event: Registered Node mamauraiserver in Controller"
I0925 00:10:55.833903    9334 shared_informer.go:277] Caches are synced for ClusterRoleAggregator
I0925 00:10:55.834458    9334 shared_informer.go:277] Caches are synced for endpoint_slice_mirroring
I0925 00:10:55.834673    9334 shared_informer.go:277] Caches are synced for ReplicationController
I0925 00:10:55.837378    9334 shared_informer.go:277] Caches are synced for HPA
I0925 00:10:55.837483    9334 shared_informer.go:277] Caches are synced for job
I0925 00:10:55.839258    9334 shared_informer.go:277] Caches are synced for persistent volume
I0925 00:10:55.856460    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:10:55.856710    9334 event.go:294] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-crd-x4ncx"
I0925 00:10:55.856840    9334 event.go:294] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helm-install-traefik-88f7w"
I0925 00:10:55.857018    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:10:55.858868    9334 shared_informer.go:277] Caches are synced for node
I0925 00:10:55.858966    9334 range_allocator.go:167] Sending events to api server.
I0925 00:10:55.859052    9334 range_allocator.go:171] Starting range CIDR allocator
I0925 00:10:55.859128    9334 shared_informer.go:270] Waiting for caches to sync for cidrallocator
I0925 00:10:55.859210    9334 shared_informer.go:277] Caches are synced for cidrallocator
I0925 00:10:55.865799    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:10:55.865954    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:10:55.866096    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:10:55.880885    9334 shared_informer.go:277] Caches are synced for certificate-csrsigning-legacy-unknown
I0925 00:10:55.881073    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:10:55.881559    9334 shared_informer.go:277] Caches are synced for TTL
I0925 00:10:55.881985    9334 shared_informer.go:277] Caches are synced for certificate-csrapproving
I0925 00:10:55.882114    9334 shared_informer.go:277] Caches are synced for ReplicaSet
I0925 00:10:55.882217    9334 shared_informer.go:277] Caches are synced for endpoint
I0925 00:10:55.882315    9334 shared_informer.go:277] Caches are synced for certificate-csrsigning-kubelet-serving
I0925 00:10:55.882417    9334 shared_informer.go:277] Caches are synced for certificate-csrsigning-kubelet-client
I0925 00:10:55.882522    9334 shared_informer.go:277] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0925 00:10:55.882622    9334 shared_informer.go:277] Caches are synced for ephemeral
I0925 00:10:55.882800    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:10:55.882912    9334 shared_informer.go:277] Caches are synced for endpoint_slice
I0925 00:10:55.883131    9334 shared_informer.go:277] Caches are synced for PVC protection
I0925 00:10:55.883265    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:10:55.883854    9334 shared_informer.go:277] Caches are synced for stateful set
I0925 00:10:55.884132    9334 shared_informer.go:277] Caches are synced for expand
I0925 00:10:55.886155    9334 range_allocator.go:372] Set node mamauraiserver PodCIDR to [10.42.0.0/24]
I0925 00:10:55.886265    9334 shared_informer.go:277] Caches are synced for disruption
I0925 00:10:55.888844    9334 shared_informer.go:277] Caches are synced for GC
time="2023-09-25T00:10:55+02:00" level=info msg="Flannel found PodCIDR assigned for node mamauraiserver"
I0925 00:10:55.889605    9334 shared_informer.go:277] Caches are synced for cronjob
time="2023-09-25T00:10:55+02:00" level=info msg="The interface ens160 with ipv4 address 192.168.141.250 will be used by flannel"
I0925 00:10:55.890578    9334 kube.go:144] Waiting 10m0s for node controller to sync
I0925 00:10:55.890887    9334 shared_informer.go:277] Caches are synced for deployment
I0925 00:10:55.891231    9334 kube.go:485] Starting kube subnet manager
I0925 00:10:55.898676    9334 shared_informer.go:277] Caches are synced for daemon sets
I0925 00:10:55.919976    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:10:55.928011    9334 controller.go:615] quota admission added evaluator for: replicasets.apps
I0925 00:10:55.929190    9334 shared_informer.go:277] Caches are synced for resource quota
I0925 00:10:55.959437    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s278n\" (UniqueName: \"kubernetes.io/projected/4396dcef-1941-4108-bfcd-111b50af2b5c-kube-api-access-s278n\") pod \"helm-install-traefik-88f7w\" (UID: \"4396dcef-1941-4108-bfcd-111b50af2b5c\") " pod="kube-system/helm-install-traefik-88f7w"
I0925 00:10:55.959654    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-values\") pod \"helm-install-traefik-crd-x4ncx\" (UID: \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\") " pod="kube-system/helm-install-traefik-crd-x4ncx"
I0925 00:10:55.959815    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-content\") pod \"helm-install-traefik-crd-x4ncx\" (UID: \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\") " pod="kube-system/helm-install-traefik-crd-x4ncx"
I0925 00:10:55.959974    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-n9d6j\" (UniqueName: \"kubernetes.io/projected/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-kube-api-access-n9d6j\") pod \"helm-install-traefik-crd-x4ncx\" (UID: \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\") " pod="kube-system/helm-install-traefik-crd-x4ncx"
I0925 00:10:55.960139    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/4396dcef-1941-4108-bfcd-111b50af2b5c-values\") pod \"helm-install-traefik-88f7w\" (UID: \"4396dcef-1941-4108-bfcd-111b50af2b5c\") " pod="kube-system/helm-install-traefik-88f7w"
I0925 00:10:55.960372    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/4396dcef-1941-4108-bfcd-111b50af2b5c-content\") pod \"helm-install-traefik-88f7w\" (UID: \"4396dcef-1941-4108-bfcd-111b50af2b5c\") " pod="kube-system/helm-install-traefik-88f7w"
I0925 00:10:55.978108    9334 shared_informer.go:277] Caches are synced for resource quota
I0925 00:10:55.978332    9334 shared_informer.go:277] Caches are synced for attach detach
I0925 00:10:55.981362    9334 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-59b4f5bbd5 to 1"
I0925 00:10:55.982404    9334 event.go:294] "Event occurred" object="kube-system/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-76d776f6f9 to 1"
I0925 00:10:55.982607    9334 event.go:294] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-7b67f64457 to 1"
I0925 00:10:55.998654    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:10:56.066564    9334 event.go:294] "Event occurred" object="kube-system/coredns-59b4f5bbd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-59b4f5bbd5-q5vsn"
I0925 00:10:56.066755    9334 event.go:294] "Event occurred" object="kube-system/metrics-server-7b67f64457" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-7b67f64457-76nbp"
I0925 00:10:56.066890    9334 event.go:294] "Event occurred" object="kube-system/local-path-provisioner-76d776f6f9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-76d776f6f9-lzsnd"
I0925 00:10:56.067029    9334 shared_informer.go:277] Caches are synced for service account
I0925 00:10:56.069746    9334 shared_informer.go:277] Caches are synced for namespace
time="2023-09-25T00:10:56+02:00" level=info msg="Starting the netpol controller version , built on , go1.19.9"
I0925 00:10:56.082356    9334 network_policy_controller.go:163] Starting network policy controller
E0925 00:10:56.108309    9334 projected.go:292] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
E0925 00:10:56.108481    9334 projected.go:198] Error preparing data for projected volume kube-api-access-s278n for pod kube-system/helm-install-traefik-88f7w: configmap "kube-root-ca.crt" not found
E0925 00:10:56.108621    9334 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/4396dcef-1941-4108-bfcd-111b50af2b5c-kube-api-access-s278n podName:4396dcef-1941-4108-bfcd-111b50af2b5c nodeName:}" failed. No retries permitted until 2023-09-25 00:10:56.608612212 +0200 CEST m=+18.682824118 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-s278n" (UniqueName: "kubernetes.io/projected/4396dcef-1941-4108-bfcd-111b50af2b5c-kube-api-access-s278n") pod "helm-install-traefik-88f7w" (UID: "4396dcef-1941-4108-bfcd-111b50af2b5c") : configmap "kube-root-ca.crt" not found
I0925 00:10:56.109232    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:10:56.109353    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:10:56.109452    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:10:56.161944    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"custom-config-volume\" (UniqueName: \"kubernetes.io/configmap/4374164a-e59e-4365-a7c1-5e93e345f4b9-custom-config-volume\") pod \"coredns-59b4f5bbd5-q5vsn\" (UID: \"4374164a-e59e-4365-a7c1-5e93e345f4b9\") " pod="kube-system/coredns-59b4f5bbd5-q5vsn"
I0925 00:10:56.162177    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-dir\" (UniqueName: \"kubernetes.io/empty-dir/b5af9cab-e632-45b9-a7d2-1fe6bfc84feb-tmp-dir\") pod \"metrics-server-7b67f64457-76nbp\" (UID: \"b5af9cab-e632-45b9-a7d2-1fe6bfc84feb\") " pod="kube-system/metrics-server-7b67f64457-76nbp"
I0925 00:10:56.162348    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rckkc\" (UniqueName: \"kubernetes.io/projected/b5af9cab-e632-45b9-a7d2-1fe6bfc84feb-kube-api-access-rckkc\") pod \"metrics-server-7b67f64457-76nbp\" (UID: \"b5af9cab-e632-45b9-a7d2-1fe6bfc84feb\") " pod="kube-system/metrics-server-7b67f64457-76nbp"
I0925 00:10:56.162516    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/4374164a-e59e-4365-a7c1-5e93e345f4b9-config-volume\") pod \"coredns-59b4f5bbd5-q5vsn\" (UID: \"4374164a-e59e-4365-a7c1-5e93e345f4b9\") " pod="kube-system/coredns-59b4f5bbd5-q5vsn"
I0925 00:10:56.162729    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-h2fl4\" (UniqueName: \"kubernetes.io/projected/4374164a-e59e-4365-a7c1-5e93e345f4b9-kube-api-access-h2fl4\") pod \"coredns-59b4f5bbd5-q5vsn\" (UID: \"4374164a-e59e-4365-a7c1-5e93e345f4b9\") " pod="kube-system/coredns-59b4f5bbd5-q5vsn"
I0925 00:10:56.162891    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/576ebc18-c4ca-4034-9a08-812593cf8b17-config-volume\") pod \"local-path-provisioner-76d776f6f9-lzsnd\" (UID: \"576ebc18-c4ca-4034-9a08-812593cf8b17\") " pod="kube-system/local-path-provisioner-76d776f6f9-lzsnd"
I0925 00:10:56.163056    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qwxqj\" (UniqueName: \"kubernetes.io/projected/576ebc18-c4ca-4034-9a08-812593cf8b17-kube-api-access-qwxqj\") pod \"local-path-provisioner-76d776f6f9-lzsnd\" (UID: \"576ebc18-c4ca-4034-9a08-812593cf8b17\") " pod="kube-system/local-path-provisioner-76d776f6f9-lzsnd"
I0925 00:10:56.169751    9334 network_policy_controller.go:175] Starting network policy controller full sync goroutine
I0925 00:10:56.424889    9334 shared_informer.go:277] Caches are synced for garbage collector
I0925 00:10:56.470244    9334 shared_informer.go:277] Caches are synced for garbage collector
I0925 00:10:56.470496    9334 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0925 00:10:56.891419    9334 kube.go:151] Node controller sync successful
I0925 00:10:56.891870    9334 vxlan.go:140] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
I0925 00:10:56.934620    9334 kube.go:506] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.42.0.0/24]
time="2023-09-25T00:10:56+02:00" level=info msg="Wrote flannel subnet file to /run/flannel/subnet.env"
time="2023-09-25T00:10:56+02:00" level=info msg="Running flannel backend."
I0925 00:10:56.961514    9334 vxlan_network.go:64] watching for new subnet leases
I0925 00:10:56.961856    9334 iptables.go:290] generated 3 rules
I0925 00:10:56.963631    9334 iptables.go:290] generated 7 rules
W0925 00:10:57.001016    9334 handler_proxy.go:100] no RequestInfo found in the context
E0925 00:10:57.001160    9334 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0925 00:10:57.001274    9334 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0925 00:10:57.001388    9334 handler_proxy.go:100] no RequestInfo found in the context
E0925 00:10:57.001499    9334 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0925 00:10:57.001651    9334 iptables.go:283] bootstrap done
I0925 00:10:57.002757    9334 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0925 00:10:57.017892    9334 iptables.go:283] bootstrap done
I0925 00:10:57.281798    9334 request.go:690] Waited for 1.017541505s due to client-side throttling, not priority and fairness, request: POST:https://127.0.0.1:6443/api/v1/namespaces/kube-system/serviceaccounts/metrics-server/token
I0925 00:11:01.914587    9334 kuberuntime_manager.go:1114] "Updating runtime config through cri with podcidr" CIDR="10.42.0.0/24"
I0925 00:11:01.915160    9334 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.42.0.0/24"
I0925 00:11:07.692875    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-59b4f5bbd5-q5vsn" podStartSLOduration=-9.22337202516192e+09 pod.CreationTimestamp="2023-09-25 00:10:56 +0200 CEST" firstStartedPulling="2023-09-25 00:11:01.689549248 +0200 CEST m=+23.763761196" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:07.641766084 +0200 CEST m=+29.715978032" watchObservedRunningTime="2023-09-25 00:11:07.692855945 +0200 CEST m=+29.767067893"
I0925 00:11:11.641177    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/local-path-provisioner-76d776f6f9-lzsnd" podStartSLOduration=-9.223372021213615e+09 pod.CreationTimestamp="2023-09-25 00:10:56 +0200 CEST" firstStartedPulling="2023-09-25 00:11:01.750902195 +0200 CEST m=+23.825114101" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:09.635081819 +0200 CEST m=+31.709293766" watchObservedRunningTime="2023-09-25 00:11:11.641159716 +0200 CEST m=+33.715371664"
I0925 00:11:12.642509    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:12.642802    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/metrics-server-7b67f64457-76nbp" podStartSLOduration=-9.223372020211992e+09 pod.CreationTimestamp="2023-09-25 00:10:56 +0200 CEST" firstStartedPulling="2023-09-25 00:11:01.675081442 +0200 CEST m=+23.749293390" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:11.642519341 +0200 CEST m=+33.716731289" watchObservedRunningTime="2023-09-25 00:11:12.642783811 +0200 CEST m=+34.716995759"
I0925 00:11:12.692390    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:12.693831    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-88f7w" podStartSLOduration=-9.22337201916096e+09 pod.CreationTimestamp="2023-09-25 00:10:55 +0200 CEST" firstStartedPulling="2023-09-25 00:11:01.74792366 +0200 CEST m=+23.822135608" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:12.643083353 +0200 CEST m=+34.717295301" watchObservedRunningTime="2023-09-25 00:11:12.693816172 +0200 CEST m=+34.768028120"
I0925 00:11:13.653544    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:13.755432    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:13.760652    9334 scope.go:115] "RemoveContainer" containerID="8eba88e22201dc00919264f46da7adb446c0d2d38c2504a4552219d0c8960633"
I0925 00:11:13.798775    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/helm-install-traefik-crd-x4ncx" podStartSLOduration=-9.223372018056017e+09 pod.CreationTimestamp="2023-09-25 00:10:55 +0200 CEST" firstStartedPulling="2023-09-25 00:11:01.744607904 +0200 CEST m=+23.818819851" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:12.694174047 +0200 CEST m=+34.768385995" watchObservedRunningTime="2023-09-25 00:11:13.798758113 +0200 CEST m=+35.872970019"
I0925 00:11:13.799694    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:13.806859    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:14.636304    9334 alloc.go:327] "allocated clusterIPs" service="kube-system/traefik" clusterIPs=map[IPv4:10.43.195.42]
I0925 00:11:14.645616    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I0925 00:11:14.703887    9334 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0925 00:11:14.704286    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set traefik-57c84cf78d to 1"
I0925 00:11:14.709855    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="AppliedDaemonSet" message="Applied LoadBalancer DaemonSet kube-system/svclb-traefik-32676a59"
I0925 00:11:14.711969    9334 event.go:294] "Event occurred" object="kube-system/traefik-57c84cf78d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: traefik-57c84cf78d-bkhzn"
I0925 00:11:14.718290    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:11:14.722177    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data\" (UniqueName: \"kubernetes.io/empty-dir/e7fb9742-6eb8-411d-9810-8b7eea76c1c8-data\") pod \"traefik-57c84cf78d-bkhzn\" (UID: \"e7fb9742-6eb8-411d-9810-8b7eea76c1c8\") " pod="kube-system/traefik-57c84cf78d-bkhzn"
I0925 00:11:14.722360    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/e7fb9742-6eb8-411d-9810-8b7eea76c1c8-tmp\") pod \"traefik-57c84cf78d-bkhzn\" (UID: \"e7fb9742-6eb8-411d-9810-8b7eea76c1c8\") " pod="kube-system/traefik-57c84cf78d-bkhzn"
I0925 00:11:14.722516    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v4hn9\" (UniqueName: \"kubernetes.io/projected/e7fb9742-6eb8-411d-9810-8b7eea76c1c8-kube-api-access-v4hn9\") pod \"traefik-57c84cf78d-bkhzn\" (UID: \"e7fb9742-6eb8-411d-9810-8b7eea76c1c8\") " pod="kube-system/traefik-57c84cf78d-bkhzn"
I0925 00:11:14.738995    9334 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0925 00:11:14.879594    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:14.879761    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:14.939559    9334 event.go:294] "Event occurred" object="kube-system/svclb-traefik-32676a59" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: svclb-traefik-32676a59-j2dgz"
I0925 00:11:14.951221    9334 topology_manager.go:210] "Topology Admit Handler"
I0925 00:11:14.964812    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:15.246309    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-n9d6j\" (UniqueName: \"kubernetes.io/projected/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-kube-api-access-n9d6j\") pod \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\" (UID: \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\") "
I0925 00:11:15.246922    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-content\") pod \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\" (UID: \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\") "
I0925 00:11:15.247220    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-values\") pod \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\" (UID: \"dd4b17ed-787e-4d76-b5a5-1b4823a60f3e\") "
W0925 00:11:15.247125    9334 empty_dir.go:525] Warning: Failed to clear quota on /var/lib/kubelet/pods/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e/volumes/kubernetes.io~configmap/content: clearQuota called, but quotas disabled
I0925 00:11:15.248577    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-content" (OuterVolumeSpecName: "content") pod "dd4b17ed-787e-4d76-b5a5-1b4823a60f3e" (UID: "dd4b17ed-787e-4d76-b5a5-1b4823a60f3e"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0925 00:11:15.248206    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-kube-api-access-n9d6j" (OuterVolumeSpecName: "kube-api-access-n9d6j") pod "dd4b17ed-787e-4d76-b5a5-1b4823a60f3e" (UID: "dd4b17ed-787e-4d76-b5a5-1b4823a60f3e"). InnerVolumeSpecName "kube-api-access-n9d6j". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0925 00:11:15.248511    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-values" (OuterVolumeSpecName: "values") pod "dd4b17ed-787e-4d76-b5a5-1b4823a60f3e" (UID: "dd4b17ed-787e-4d76-b5a5-1b4823a60f3e"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0925 00:11:15.347871    9334 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-n9d6j\" (UniqueName: \"kubernetes.io/projected/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-kube-api-access-n9d6j\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:11:15.348163    9334 reconciler_common.go:295] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-values\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:11:15.348345    9334 reconciler_common.go:295] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/dd4b17ed-787e-4d76-b5a5-1b4823a60f3e-content\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:11:15.946521    9334 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0a217f3710c07a592fca61cece0a33aa58d350c016ee766155bd0b34f4cbbfc3"
I0925 00:11:15.969549    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:16.020465    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:16.705744    9334 controller.go:615] quota admission added evaluator for: ingressroutes.traefik.containo.us
I0925 00:11:16.961421    9334 scope.go:115] "RemoveContainer" containerID="8eba88e22201dc00919264f46da7adb446c0d2d38c2504a4552219d0c8960633"
I0925 00:11:16.983028    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:16.988567    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:16.994897    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:17.002940    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik-crd
I0925 00:11:17.003165    9334 event.go:294] "Event occurred" object="kube-system/helm-install-traefik-crd" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0925 00:11:17.992679    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:17.998208    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:18.167885    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"content\" (UniqueName: \"kubernetes.io/configmap/4396dcef-1941-4108-bfcd-111b50af2b5c-content\") pod \"4396dcef-1941-4108-bfcd-111b50af2b5c\" (UID: \"4396dcef-1941-4108-bfcd-111b50af2b5c\") "
I0925 00:11:18.168450    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"values\" (UniqueName: \"kubernetes.io/secret/4396dcef-1941-4108-bfcd-111b50af2b5c-values\") pod \"4396dcef-1941-4108-bfcd-111b50af2b5c\" (UID: \"4396dcef-1941-4108-bfcd-111b50af2b5c\") "
I0925 00:11:18.169103    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-s278n\" (UniqueName: \"kubernetes.io/projected/4396dcef-1941-4108-bfcd-111b50af2b5c-kube-api-access-s278n\") pod \"4396dcef-1941-4108-bfcd-111b50af2b5c\" (UID: \"4396dcef-1941-4108-bfcd-111b50af2b5c\") "
W0925 00:11:18.168150    9334 empty_dir.go:525] Warning: Failed to clear quota on /var/lib/kubelet/pods/4396dcef-1941-4108-bfcd-111b50af2b5c/volumes/kubernetes.io~configmap/content: clearQuota called, but quotas disabled
I0925 00:11:18.175275    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/4396dcef-1941-4108-bfcd-111b50af2b5c-content" (OuterVolumeSpecName: "content") pod "4396dcef-1941-4108-bfcd-111b50af2b5c" (UID: "4396dcef-1941-4108-bfcd-111b50af2b5c"). InnerVolumeSpecName "content". PluginName "kubernetes.io/configmap", VolumeGidValue ""
I0925 00:11:18.172841    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4396dcef-1941-4108-bfcd-111b50af2b5c-values" (OuterVolumeSpecName: "values") pod "4396dcef-1941-4108-bfcd-111b50af2b5c" (UID: "4396dcef-1941-4108-bfcd-111b50af2b5c"). InnerVolumeSpecName "values". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0925 00:11:18.173983    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4396dcef-1941-4108-bfcd-111b50af2b5c-kube-api-access-s278n" (OuterVolumeSpecName: "kube-api-access-s278n") pod "4396dcef-1941-4108-bfcd-111b50af2b5c" (UID: "4396dcef-1941-4108-bfcd-111b50af2b5c"). InnerVolumeSpecName "kube-api-access-s278n". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0925 00:11:18.273549    9334 reconciler_common.go:295] "Volume detached for volume \"values\" (UniqueName: \"kubernetes.io/secret/4396dcef-1941-4108-bfcd-111b50af2b5c-values\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:11:18.273928    9334 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-s278n\" (UniqueName: \"kubernetes.io/projected/4396dcef-1941-4108-bfcd-111b50af2b5c-kube-api-access-s278n\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:11:18.274132    9334 reconciler_common.go:295] "Volume detached for volume \"content\" (UniqueName: \"kubernetes.io/configmap/4396dcef-1941-4108-bfcd-111b50af2b5c-content\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:11:18.969889    9334 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8f86d0f0f2285d18b7b232eeca78fc5b1896d8bccd50530d4438a41673de7f56"
I0925 00:11:19.026975    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:20.034081    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:20.037957    9334 event.go:294] "Event occurred" object="kube-system/traefik" fieldPath="" kind="Service" apiVersion="v1" type="Normal" reason="UpdatedLoadBalancer" message="Updated LoadBalancer with new IPs: [] -> [192.168.56.110]"
I0925 00:11:20.040165    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/svclb-traefik-32676a59-j2dgz" podStartSLOduration=-9.223372030814627e+09 pod.CreationTimestamp="2023-09-25 00:11:14 +0200 CEST" firstStartedPulling="2023-09-25 00:11:15.20444422 +0200 CEST m=+37.278656168" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:19.986456342 +0200 CEST m=+42.060668290" watchObservedRunningTime="2023-09-25 00:11:20.040149411 +0200 CEST m=+42.114361358"
I0925 00:11:20.040814    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:20.044339    9334 job_controller.go:514] enqueueing job kube-system/helm-install-traefik
I0925 00:11:20.044550    9334 event.go:294] "Event occurred" object="kube-system/helm-install-traefik" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0925 00:11:24.011802    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/traefik-57c84cf78d-bkhzn" podStartSLOduration=-9.223372026843023e+09 pod.CreationTimestamp="2023-09-25 00:11:14 +0200 CEST" firstStartedPulling="2023-09-25 00:11:15.183753268 +0200 CEST m=+37.257965216" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:11:22.015941733 +0200 CEST m=+44.090153681" watchObservedRunningTime="2023-09-25 00:11:24.011753426 +0200 CEST m=+46.085965373"
E0925 00:11:25.989891    9334 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0925 00:11:25.990188    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for tlsstores.traefik.containo.us
I0925 00:11:25.990445    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for middlewaretcps.traefik.containo.us
I0925 00:11:25.990596    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for traefikservices.traefik.containo.us
I0925 00:11:25.990744    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for serverstransports.traefik.containo.us
I0925 00:11:25.990891    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for ingressroutetcps.traefik.containo.us
I0925 00:11:25.991034    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for ingressroutes.traefik.containo.us
I0925 00:11:25.991179    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for middlewares.traefik.containo.us
I0925 00:11:25.991332    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for tlsoptions.traefik.containo.us
I0925 00:11:25.991475    9334 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for ingressrouteudps.traefik.containo.us
I0925 00:11:25.991652    9334 shared_informer.go:270] Waiting for caches to sync for resource quota
I0925 00:11:26.092984    9334 shared_informer.go:277] Caches are synced for resource quota
W0925 00:11:26.459123    9334 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E0925 00:11:26.477295    9334 memcache.go:287] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E0925 00:11:26.478384    9334 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0925 00:11:26.479421    9334 shared_informer.go:270] Waiting for caches to sync for garbage collector
I0925 00:11:26.479632    9334 shared_informer.go:277] Caches are synced for garbage collector
I0925 00:13:50.518230    9334 event.go:294] "Event occurred" object="default/app1" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app1-5945bc7df8 to 1"
I0925 00:13:50.523805    9334 event.go:294] "Event occurred" object="default/app1-5945bc7df8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app1-5945bc7df8-b7nfz"
I0925 00:13:50.529632    9334 topology_manager.go:210] "Topology Admit Handler"
E0925 00:13:50.529788    9334 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4396dcef-1941-4108-bfcd-111b50af2b5c" containerName="helm"
E0925 00:13:50.529900    9334 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="dd4b17ed-787e-4d76-b5a5-1b4823a60f3e" containerName="helm"
I0925 00:13:50.530013    9334 memory_manager.go:346] "RemoveStaleState removing state" podUID="4396dcef-1941-4108-bfcd-111b50af2b5c" containerName="helm"
I0925 00:13:50.530116    9334 memory_manager.go:346] "RemoveStaleState removing state" podUID="dd4b17ed-787e-4d76-b5a5-1b4823a60f3e" containerName="helm"
I0925 00:13:50.552798    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ttc49\" (UniqueName: \"kubernetes.io/projected/44906d73-f6ba-4f22-addf-0da9afa0c9b4-kube-api-access-ttc49\") pod \"app1-5945bc7df8-b7nfz\" (UID: \"44906d73-f6ba-4f22-addf-0da9afa0c9b4\") " pod="default/app1-5945bc7df8-b7nfz"
I0925 00:14:14.365853    9334 alloc.go:327] "allocated clusterIPs" service="default/service-app1" clusterIPs=map[IPv4:10.43.123.183]
I0925 00:14:23.706223    9334 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io
time="2023-09-25T00:15:38+02:00" level=info msg="COMPACT revision 0 has already been compacted"
W0925 00:15:41.430182    9334 machine.go:65] Cannot read vendor id correctly, set empty.
I0925 00:16:24.394387    9334 event.go:294] "Event occurred" object="default/app1" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app1-5c846d64d4 to 1"
I0925 00:16:24.401268    9334 event.go:294] "Event occurred" object="default/app1-5c846d64d4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app1-5c846d64d4-vg95j"
I0925 00:16:24.406222    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app1-5945bc7df8-b7nfz" podStartSLOduration=-9.22337188244857e+09 pod.CreationTimestamp="2023-09-25 00:13:50 +0200 CEST" firstStartedPulling="2023-09-25 00:13:50.994197918 +0200 CEST m=+193.068409824" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:14:00.587081166 +0200 CEST m=+202.661293072" watchObservedRunningTime="2023-09-25 00:16:24.406204847 +0200 CEST m=+346.480416753"
I0925 00:16:24.406994    9334 topology_manager.go:210] "Topology Admit Handler"
E0925 00:16:24.407106    9334 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4396dcef-1941-4108-bfcd-111b50af2b5c" containerName="helm"
I0925 00:16:24.407221    9334 memory_manager.go:346] "RemoveStaleState removing state" podUID="4396dcef-1941-4108-bfcd-111b50af2b5c" containerName="helm"
I0925 00:16:24.569502    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4h9xf\" (UniqueName: \"kubernetes.io/projected/e138f814-6c33-42fd-a0dd-34670a7c05dc-kube-api-access-4h9xf\") pod \"app1-5c846d64d4-vg95j\" (UID: \"e138f814-6c33-42fd-a0dd-34670a7c05dc\") " pod="default/app1-5c846d64d4-vg95j"
I0925 00:16:24.569905    9334 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"nginx-index\" (UniqueName: \"kubernetes.io/configmap/e138f814-6c33-42fd-a0dd-34670a7c05dc-nginx-index\") pod \"app1-5c846d64d4-vg95j\" (UID: \"e138f814-6c33-42fd-a0dd-34670a7c05dc\") " pod="default/app1-5c846d64d4-vg95j"
I0925 00:16:27.246761    9334 event.go:294] "Event occurred" object="default/app1" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set app1-5945bc7df8 to 0 from 1"
I0925 00:16:27.255422    9334 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app1-5c846d64d4-vg95j" podStartSLOduration=-9.223372033599373e+09 pod.CreationTimestamp="2023-09-25 00:16:24 +0200 CEST" firstStartedPulling="2023-09-25 00:16:24.838652885 +0200 CEST m=+346.912864791" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-09-25 00:16:27.176123139 +0200 CEST m=+349.250335087" watchObservedRunningTime="2023-09-25 00:16:27.25540331 +0200 CEST m=+349.329615216"
I0925 00:16:27.260664    9334 event.go:294] "Event occurred" object="default/app1-5945bc7df8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: app1-5945bc7df8-b7nfz"
I0925 00:16:27.696651    9334 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-ttc49\" (UniqueName: \"kubernetes.io/projected/44906d73-f6ba-4f22-addf-0da9afa0c9b4-kube-api-access-ttc49\") pod \"44906d73-f6ba-4f22-addf-0da9afa0c9b4\" (UID: \"44906d73-f6ba-4f22-addf-0da9afa0c9b4\") "
I0925 00:16:27.702345    9334 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/44906d73-f6ba-4f22-addf-0da9afa0c9b4-kube-api-access-ttc49" (OuterVolumeSpecName: "kube-api-access-ttc49") pod "44906d73-f6ba-4f22-addf-0da9afa0c9b4" (UID: "44906d73-f6ba-4f22-addf-0da9afa0c9b4"). InnerVolumeSpecName "kube-api-access-ttc49". PluginName "kubernetes.io/projected", VolumeGidValue ""
I0925 00:16:27.798551    9334 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-ttc49\" (UniqueName: \"kubernetes.io/projected/44906d73-f6ba-4f22-addf-0da9afa0c9b4-kube-api-access-ttc49\") on node \"mamauraiserver\" DevicePath \"\""
I0925 00:16:28.162022    9334 scope.go:115] "RemoveContainer" containerID="bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149"
I0925 00:16:28.195880    9334 scope.go:115] "RemoveContainer" containerID="bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149"
E0925 00:16:28.201129    9334 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149\": not found" containerID="bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149"
I0925 00:16:28.201768    9334 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:containerd ID:bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149} err="failed to get container status \"bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149\": rpc error: code = NotFound desc = an error occurred when try to find container \"bddbdfa87c551b8448d3d942f511b79b61b6c1edd3fa8d1245d5a209f49a8149\": not found"
I0925 00:16:29.532014    9334 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=44906d73-f6ba-4f22-addf-0da9afa0c9b4 path="/var/lib/kubelet/pods/44906d73-f6ba-4f22-addf-0da9afa0c9b4/volumes"
time="2023-09-25T00:20:38+02:00" level=info msg="COMPACT revision 0 has already been compacted"
W0925 00:20:41.429949    9334 machine.go:65] Cannot read vendor id correctly, set empty.
time="2023-09-25T00:25:38+02:00" level=info msg="COMPACT revision 0 has already been compacted"
W0925 00:25:41.430525    9334 machine.go:65] Cannot read vendor id correctly, set empty.
